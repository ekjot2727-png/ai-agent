# AutoOps AI - Data Pipeline Workflow
# This workflow orchestrates data extraction, transformation, and loading

id: autoops-data-pipeline
namespace: autoops.data

description: |
  Automated data pipeline workflow for ETL operations.
  Generated by AutoOps AI Agent System.

inputs:
  - id: source
    type: STRING
    required: true
    description: Data source identifier
  - id: destination
    type: STRING
    required: true
    description: Data destination identifier
  - id: batch_size
    type: INT
    defaults: 1000
    description: Number of records per batch

variables:
  timestamp: "{{ now() }}"
  run_id: "{{ execution.id }}"

tasks:
  - id: validate_inputs
    type: io.kestra.core.tasks.scripts.Bash
    description: Validate input parameters
    commands:
      - echo "Validating inputs..."
      - echo "Source: {{ inputs.source }}"
      - echo "Destination: {{ inputs.destination }}"
      - echo "Batch size: {{ inputs.batch_size }}"

  - id: extract_data
    type: io.kestra.core.tasks.scripts.Python
    description: Extract data from source
    dependsOn:
      - validate_inputs
    inputFiles:
      main.py: |
        import json
        
        # Simulated data extraction
        data = {
            "source": "{{ inputs.source }}",
            "records": [
                {"id": i, "value": f"record_{i}"} 
                for i in range({{ inputs.batch_size }})
            ],
            "timestamp": "{{ variables.timestamp }}"
        }
        
        print(f"Extracted {len(data['records'])} records")
        print(json.dumps(data, indent=2))
    commands:
      - python main.py

  - id: transform_data
    type: io.kestra.core.tasks.scripts.Python
    description: Transform and validate data
    dependsOn:
      - extract_data
    inputFiles:
      transform.py: |
        import json
        
        # Simulated transformation
        print("Applying transformations...")
        print("- Normalizing field names")
        print("- Validating data types")
        print("- Filtering invalid records")
        print("Transformation complete")
    commands:
      - python transform.py

  - id: load_data
    type: io.kestra.core.tasks.scripts.Python
    description: Load data to destination
    dependsOn:
      - transform_data
    inputFiles:
      load.py: |
        print("Loading data to {{ inputs.destination }}...")
        print("- Establishing connection")
        print("- Inserting records in batches")
        print("- Verifying data integrity")
        print("Load complete: {{ inputs.batch_size }} records inserted")
    commands:
      - python load.py

  - id: generate_report
    type: io.kestra.core.tasks.scripts.Bash
    description: Generate execution report
    dependsOn:
      - load_data
    commands:
      - echo "=== Pipeline Execution Report ==="
      - echo "Run ID: {{ variables.run_id }}"
      - echo "Source: {{ inputs.source }}"
      - echo "Destination: {{ inputs.destination }}"
      - echo "Records processed: {{ inputs.batch_size }}"
      - echo "Status: SUCCESS"
      - echo "================================="

triggers:
  - id: schedule
    type: io.kestra.core.models.triggers.types.Schedule
    cron: "0 2 * * *"  # Run daily at 2 AM
    inputs:
      source: "production_db"
      destination: "data_warehouse"
      batch_size: 10000

errors:
  - id: notify_failure
    type: io.kestra.core.tasks.scripts.Bash
    commands:
      - echo "Pipeline failed! Sending notification..."
      - echo "Error details logged for investigation"
